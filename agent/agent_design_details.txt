================================================================================
AGENT - Design Details
================================================================================
SafeClaw Agent Sub-Project
================================================================================

OVERVIEW
--------
The Agent is the "Mind" of SafeClaw. It runs as a separate, self-contained process.
It implements the Think-Act-Observe loop: reads user input, builds a prompt from
context (SOUL, memory, artifact, history), sends to an LLM (Ollama), parses the
response for <tool_code> actions, and executes them. Agent actions run locally;
Router actions are pushed to Redis and the Agent waits for the response.


DIRECTORY STRUCTURE
-------------------

agent/
├── start_agent.py            # Entry point. Loads config, BaseAgent + channels (Console, Telegram, etc.)
├── config.json               # Runtime config (gitignored). Channels, tokens, etc.
├── config_initial.json       # Template. Copy to config.json on first run.
├── .env                     # REDIS_URL, REMOTE_BROWSER_SERVER, COMMAND_QUEUE, RESPONSE_PREFIX
├── requirements.txt         # ollama, redis, python-dotenv
├── README.md
├── agent_design_details.txt
│
├── libs/
│   ├── agent_config.py     # AgentConfig: load config, interactive prompts (./start_agent.py config <key>)
│   ├── action_executor.py  # ActionExecutor: runs agent actions, pushes router actions
│   └── remote_chrome_utils.py  # dismiss_consent() for BROWSER_VISION
│
├── llm/                     # LLM providers (like channel structure)
│   ├── base_llm.py         # BaseLLM: prompt, parse, process_turn. Subclasses implement chat()
│   ├── ollama/
│   │   ├── llm.py          # OllamaLLM: local Ollama connection
│   │   └── OLLAMA_SETUP.md
│
├── workspace/               # Runtime context and config
│   ├── PROMPT.md            # Main prompt template (placeholders)
│   ├── SOUL.md              # Identity, tone, ethical boundaries
│   ├── memory.json          # Persistent key-value memory
│   ├── agent_action.json    # Agent actions the LLM can call (local)
│   ├── router_actions.json  # Router actions the LLM can call (via queue)
│   ├── input_history.json   # Last 10 user/response pairs
│   ├── artifact.json        # Latest action results, follow-ups
│   └── output/              # Generated files (prompt_cache, browser_vision.*)
│
└── xxxxxbrain/, xxxxxear/, xxxxxmouth/, ...  # (Reserved) Future modular senses


COMPONENTS
----------

1. start_agent.py (main loop)
   - Loads input_history, builds prompt via Prompt.create_prompt()
   - Sends to Ollama, parses LLMResponse (message + actions)
   - For each action: ActionExecutor(action, params, workspace).execute()
   - Handles follow-up actions (e.g. BROWSER_VISION -> _LLM_SUMMARY)
   - Saves artifact, updates input_history

2. llm/base_llm.py (BaseLLM: prompt, parse, process_turn)
   - create_prompt(user_input) -> merged prompt string
   - Loads PROMPT.md, replaces placeholders:
     {{SOUL_CONTENT}}     <- SOUL.md
     {{MEMORY_CONTENT}}   <- memory.json
     {{ARTIFACT}}         <- artifact.json
     {{USER_INPUT_HISTORY}} <- input_history.json
     {{AGENT_ACTIONS}}    <- agent_action.json
     {{ROUTER_ACTIONS}}   <- router_actions.json
     {{USER_MESSAGE}}     <- escaped user input
   - Writes result to workspace/output/prompt_cache.txt

3. llm/base_llm.py (response parsing)
   - Parses LLM output for <tool_code>...</tool_code>
   - Extracts: message (text before tag), actions (JSON array)
   - Raises LLMResponseError if invalid

4. libs/action_executor.py (ActionExecutor)
   - execute() branches:
     A) AGENT ACTION: Has method _ACTION_NAME -> run locally
     B) ROUTER ACTION: No local method -> push to Redis, BLPOP response (timeout from config.json)
   - Agent actions: _MEMORY_WRITE, _BROWSER_VISION, _LLM_SUMMARY
   - Router actions: any name in router_actions.json (e.g. CREATE_POST)

5. libs/agent_config.py (AgentConfig)
   - load_config(): Load config.json, clone from config_initial.json if missing
   - run_interactive(key): Interactive prompts for config keys (timeout, llm)
   - Usage: ./start_agent.py config timeout | ./start_agent.py config llm

6. libs/remote_chrome_utils.py
   - dismiss_consent(driver): Clicks cookie/consent buttons before screenshot


AGENT ACTIONS (local)
---------------------

_MEMORY_WRITE
  params: { "new_memory": { "KEY": "value", ... } }
  Merges into memory.json, returns confirmation.

_BROWSER_VISION
  params: { "url": "https://..." }
  Uses Selenium + REMOTE_BROWSER_SERVER. Saves html, png, txt to workspace/output/.
  Returns follow_up: _LLM_SUMMARY with content path.

_LLM_SUMMARY
  params: { "content": "<path to txt>" }
  Summarizes file content via Ollama. Returns { "output": "..." }.


ROUTER ACTIONS (via Redis)
--------------------------

Agent pushes to COMMAND_QUEUE, starts listener on RESPONSE_PREFIX+message_id.
Router consumes, runs skill, pushes result. Agent receives within 10s or times out.

Example: CREATE_POST
  params: { "platform": "X", "text": "..." }
  Router skill executes, returns { "status": "..." }.


PROMPT TEMPLATE (PROMPT.md)
---------------------------

Placeholders:
  {{SOUL_CONTENT}}       - Identity from SOUL.md
  {{MEMORY_CONTENT}}     - memory.json
  {{ARTIFACT}}           - artifact.json (last action results)
  {{USER_INPUT_HISTORY}} - input_history.json
  {{AGENT_ACTIONS}}      - agent_action.json (tool definitions)
  {{ROUTER_ACTIONS}}     - router_actions.json
  {{USER_MESSAGE}}       - Current user input (escaped)


LLM OUTPUT FORMAT
-----------------

The LLM must respond with:
  <optional message text><tool_code>[{"name": "ACTION", "params": {...}}, ...]</tool_code>

If no tool needed: plain text only, no <tool_code>.


CONFIGURATION
-------------

config.json (llm, timeout, channels):
  - llm.provider, llm.model  - LLM provider and model (default: ollama, llama3.1:8B)
  - timeout                  - Message age and response queue timeout in seconds (default: 10)
  - channels                 - Telegram, etc. (see channel/TELEGRAM_SETUP.md)

Interactive config: ./start_agent.py config <key>
  - ./start_agent.py config timeout
  - ./start_agent.py config llm

.env (secrets, Redis, browser):
  REDIS_URL              - Redis connection (for router actions)
  REMOTE_BROWSER_SERVER  - Selenium remote Chrome URL (e.g. http://host:4444)
  COMMAND_QUEUE          - Queue for router commands (default: safeclaw:command_queue)
  RESPONSE_PREFIX        - Response key prefix (default: safeclaw:response:)

For openai: OPENAI_API_KEY
For gemini: GOOGLE_API_KEY or GEMINI_API_KEY


ADDING A NEW AGENT ACTION
-------------------------

1. Add entry to workspace/agent_action.json with name, instruction, params schema.
2. Add method _ACTION_NAME(self, params) to libs/action_executor.py.
3. Return dict (merged into artifact).


ADDING A NEW ROUTER ACTION
-------------------------

1. Add entry to workspace/router_actions.json.
2. Create skill in router/skills/{action_name}/skill.py.
3. Agent will push to queue; Router will route and execute.


RUN
---

From agent directory:
  python start_agent.py

Or from project root:
  python agent/start_agent.py

Commands:
  ./start_agent.py           - Start the agent
  ./start_agent.py clear    - Clear workspace (artifact, input_history, log)
  ./start_agent.py config [key] - Interactive config. Keys: timeout, llm

Requires: LLM (ollama local, or openai/gemini with API key).
For router actions: Redis + Router process.
For BROWSER_VISION: REMOTE_BROWSER_SERVER (Selenium Grid / remote Chrome).
